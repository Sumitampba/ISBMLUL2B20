{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46af602d-fbe4-4d47-aab3-35d85e50f2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.4934, Prediction = 0.2976\n",
      "Epoch 11: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 21: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 31: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 41: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 51: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 61: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 71: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 81: Loss = 0.0000, Prediction = 1.0000\n",
      "Epoch 91: Loss = 0.0000, Prediction = 1.0000\n",
      "\n",
      "Final Weights:\n",
      "W1 = 0.1622, W2 = 0.3331, W3 = 0.2344, W4 = 0.2762, W5 = 0.2996, W6 = 0.3511\n",
      "\n",
      "Final Prediction (ReLU): 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6):\n",
    "    # Calculate hidden neurons\n",
    "    h1 = i1 * W1 + i2 * W3\n",
    "    h2 = i1 * W2 + i2 * W4\n",
    "    \n",
    "    # Apply ReLU activation to hidden neurons\n",
    "    h1_out = relu(h1)\n",
    "    h2_out = relu(h2)\n",
    "    \n",
    "    # Calculate output neuron\n",
    "    out = h1_out * W5 + h2_out * W6\n",
    "    \n",
    "    return out, h1_out, h2_out\n",
    "\n",
    "# Mean Squared Error (MSE) loss function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "# Derivative of MSE loss with respect to predicted output\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred)\n",
    "\n",
    "# Backpropagation to update weights\n",
    "def backpropagation_update_weights(i1, i2, h1_out, h2_out, W5, W6, out_pred, y_true, learning_rate):\n",
    "    # Compute output error\n",
    "    delta = mse_loss_derivative(y_true, out_pred)\n",
    "    \n",
    "    # Update output layer weights\n",
    "    delta_W5 = learning_rate * delta * h1_out\n",
    "    delta_W6 = learning_rate * delta * h2_out\n",
    "    \n",
    "    # Compute hidden layer errors\n",
    "    delta_h1 = delta * W5\n",
    "    delta_h2 = delta * W6\n",
    "    \n",
    "    # Update hidden layer weights\n",
    "    delta_W1 = learning_rate * delta_h1 * i1\n",
    "    delta_W2 = learning_rate * delta_h2 * i1\n",
    "    delta_W3 = learning_rate * delta_h1 * i2\n",
    "    delta_W4 = learning_rate * delta_h2 * i2\n",
    "    \n",
    "    return delta_W1, delta_W2, delta_W3, delta_W4, delta_W5, delta_W6\n",
    "\n",
    "# Initial weights\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Inputs\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "\n",
    "# True output\n",
    "y_true = 1\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Number of iterations (epochs) for training\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    out_pred, h1_out, h2_out = forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6)\n",
    "    \n",
    "    # Compute MSE loss\n",
    "    loss = mean_squared_error(y_true, out_pred)\n",
    "    \n",
    "    # Backpropagation to update weights\n",
    "    delta_W1, delta_W2, delta_W3, delta_W4, delta_W5, delta_W6 = backpropagation_update_weights(\n",
    "        i1, i2, h1_out, h2_out, W5, W6, out_pred, y_true, learning_rate)\n",
    "    \n",
    "    # Update weights\n",
    "    W1 -= delta_W1\n",
    "    W2 -= delta_W2\n",
    "    W3 -= delta_W3\n",
    "    W4 -= delta_W4\n",
    "    W5 -= delta_W5\n",
    "    W6 -= delta_W6\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}, Prediction = {out_pred:.4f}\")\n",
    "\n",
    "# Final weights after training\n",
    "print(\"\\nFinal Weights:\")\n",
    "print(f\"W1 = {W1:.4f}, W2 = {W2:.4f}, W3 = {W3:.4f}, W4 = {W4:.4f}, W5 = {W5:.4f}, W6 = {W6:.4f}\")\n",
    "\n",
    "# Perform final forward propagation with ReLU activation\n",
    "final_out_pred, _, _ = forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6)\n",
    "print(f\"\\nFinal Prediction (ReLU): {final_out_pred:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e312521a-1431-42e1-a2cb-ae7930ff74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.2467\n",
      "\n",
      "Final Weights:\n",
      "W1 = 0.1618, W2 = 0.3341, W3 = 0.2337, W4 = 0.2783, W5 = 0.3006, W6 = 0.3490\n",
      "\n",
      "Final Prediction: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function (ReLU)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6):\n",
    "    # Calculate hidden neurons\n",
    "    h1 = i1 * W1 + i2 * W3\n",
    "    h2 = i1 * W2 + i2 * W4\n",
    "    \n",
    "    # Apply ReLU activation to hidden neurons\n",
    "    h1_out = relu(h1)\n",
    "    h2_out = relu(h2)\n",
    "    \n",
    "    # Calculate output neuron\n",
    "    out = h1_out * W5 + h2_out * W6\n",
    "    \n",
    "    return out, h1_out, h2_out\n",
    "\n",
    "# Mean Squared Error (MSE) loss\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return 0.5 * (y_true - y_pred) ** 2\n",
    "\n",
    "# Backpropagation to compute gradients and update weights\n",
    "def backpropagation(i1, i2, W1, W2, W3, W4, W5, W6, y_true, learning_rate):\n",
    "    # Forward propagation to get predictions and hidden neuron outputs\n",
    "    out_pred, h1_out, h2_out = forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = mean_squared_error(y_true, out_pred)\n",
    "    \n",
    "    # Compute derivative of loss w.r.t. output\n",
    "    dL_dout = -(y_true - out_pred)\n",
    "    \n",
    "    # Gradients for output neuron weights (W5 and W6)\n",
    "    dL_dW5 = dL_dout * h1_out\n",
    "    dL_dW6 = dL_dout * h2_out\n",
    "    \n",
    "    # Gradients for hidden neuron outputs (h1_out and h2_out)\n",
    "    dL_dh1_out = dL_dout * W5\n",
    "    dL_dh2_out = dL_dout * W6\n",
    "    \n",
    "    # Gradients for hidden neuron inputs (h1 and h2)\n",
    "    dL_dh1 = dL_dh1_out * relu_derivative(i1 * W1 + i2 * W3)\n",
    "    dL_dh2 = dL_dh2_out * relu_derivative(i1 * W2 + i2 * W4)\n",
    "    \n",
    "    # Gradients for input-to-hidden weights (W1, W2, W3, W4)\n",
    "    dL_dW1 = dL_dh1 * i1\n",
    "    dL_dW2 = dL_dh2 * i1\n",
    "    dL_dW3 = dL_dh1 * i2\n",
    "    dL_dW4 = dL_dh2 * i2\n",
    "    \n",
    "    # Update weights using gradients and learning rate\n",
    "    W1 -= learning_rate * dL_dW1\n",
    "    W2 -= learning_rate * dL_dW2\n",
    "    W3 -= learning_rate * dL_dW3\n",
    "    W4 -= learning_rate * dL_dW4\n",
    "    W5 -= learning_rate * dL_dW5\n",
    "    W6 -= learning_rate * dL_dW6\n",
    "    \n",
    "    return W1, W2, W3, W4, W5, W6, loss\n",
    "\n",
    "# Initial weights\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Inputs and true output\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "y_true = 1\n",
    "\n",
    "# Learning rate and number of epochs\n",
    "learning_rate = 0.05\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Perform backpropagation to update weights\n",
    "    W1, W2, W3, W4, W5, W6, loss = backpropagation(i1, i2, W1, W2, W3, W4, W5, W6, y_true, learning_rate)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Output final weights\n",
    "print(\"\\nFinal Weights:\")\n",
    "print(f\"W1 = {W1:.4f}, W2 = {W2:.4f}, W3 = {W3:.4f}, W4 = {W4:.4f}, W5 = {W5:.4f}, W6 = {W6:.4f}\")\n",
    "\n",
    "# Perform final forward propagation to get prediction\n",
    "final_out_pred, _, _ = forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6)\n",
    "print(f\"\\nFinal Prediction: {final_out_pred:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abadbf4-ac74-4eb4-8f87-5ca1d0372694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Weights:\n",
      "W1 = 0.1126\n",
      "W2 = 0.2812\n",
      "W3 = 0.1353\n",
      "W4 = 0.1725\n",
      "W5 = 0.2025\n",
      "W6 = 0.2000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function (ReLU)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Forward propagation to compute loss and gradients\n",
    "def forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6, y_true):\n",
    "    # Calculate hidden neurons\n",
    "    h1 = i1 * W1 + i2 * W3\n",
    "    h2 = i1 * W2 + i2 * W4\n",
    "    \n",
    "    # Apply ReLU activation to hidden neurons\n",
    "    h1_out = relu(h1)\n",
    "    h2_out = relu(h2)\n",
    "    \n",
    "    # Calculate output neuron\n",
    "    out = h1_out * W5 + h2_out * W6\n",
    "    \n",
    "    # Compute Mean Squared Error (MSE) loss\n",
    "    loss = 0.5 * (y_true - out) ** 2\n",
    "    \n",
    "    # Compute gradients for weights using backpropagation\n",
    "    dL_dW1 = - (y_true - out) * W5 * relu_derivative(h1) * i1\n",
    "    dL_dW2 = - (y_true - out) * W6 * relu_derivative(h2) * i1\n",
    "    dL_dW3 = - (y_true - out) * W5 * relu_derivative(h1) * i2\n",
    "    dL_dW4 = - (y_true - out) * W6 * relu_derivative(h2) * i2\n",
    "    dL_dW5 = - (y_true - out) * h1_out\n",
    "    dL_dW6 = - (y_true - out) * h2_out\n",
    "    \n",
    "    return loss, dL_dW1, dL_dW2, dL_dW3, dL_dW4, dL_dW5, dL_dW6\n",
    "\n",
    "# Initial weights\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Inputs and true output\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "y_true = 1\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Perform one iteration of gradient descent\n",
    "loss, dL_dW1, dL_dW2, dL_dW3, dL_dW4, dL_dW5, dL_dW6 = forward_propagation(i1, i2, W1, W2, W3, W4, W5, W6, y_true)\n",
    "\n",
    "# Update weights using gradients and learning rate\n",
    "W1 -= learning_rate * dL_dW1\n",
    "W2 -= learning_rate * dL_dW2\n",
    "W3 -= learning_rate * dL_dW3\n",
    "W4 -= learning_rate * dL_dW4\n",
    "W5 -= learning_rate * dL_dW5\n",
    "W6 -= learning_rate * dL_dW6\n",
    "\n",
    "# Print new weights\n",
    "print(\"New Weights:\")\n",
    "print(f\"W1 = {W1:.4f}\")\n",
    "print(f\"W2 = {W2:.4f}\")\n",
    "print(f\"W3 = {W3:.4f}\")\n",
    "print(f\"W4 = {W4:.4f}\")\n",
    "print(f\"W5 = {W5:.4f}\")\n",
    "print(f\"W6 = {W6:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1711ea-3c3d-49c5-a7e4-72ae9adc45ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (prediction) with ReLU activation: 0.29760000000000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function (ReLU)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Forward propagation with ReLU activation\n",
    "def forward_propagation_relu(i1, i2, W1, W2, W3, W4, W5, W6):\n",
    "    # Calculate hidden neurons\n",
    "    h1 = i1 * W1 + i2 * W3\n",
    "    h2 = i1 * W2 + i2 * W4\n",
    "    \n",
    "    # Apply ReLU activation to hidden neurons\n",
    "    h1_out = relu(h1)\n",
    "    h2_out = relu(h2)\n",
    "    \n",
    "    # Calculate output neuron\n",
    "    out = h1_out * W5 + h2_out * W6\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Initial weights\n",
    "W1 = 0.10\n",
    "W2 = 0.27\n",
    "W3 = 0.11\n",
    "W4 = 0.15\n",
    "W5 = 0.18\n",
    "W6 = 0.16\n",
    "\n",
    "# Inputs\n",
    "i1 = 2\n",
    "i2 = 4\n",
    "\n",
    "# Perform forward propagation with ReLU activation\n",
    "output = forward_propagation_relu(i1, i2, W1, W2, W3, W4, W5, W6)\n",
    "\n",
    "# Print the output representing prediction\n",
    "print(\"Output (prediction) with ReLU activation:\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
